{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a0a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Best Logistic Regression Parameters: {'C': 1000}\n",
      "Best CV Accuracy (LogReg): 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'C': 100}\n",
      "Best CV Accuracy (SVM): 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96919ee139ce41d8a70d78580d2cec2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 60/60 [00:39<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to data/results/logreg_predictions.csv\n",
      "✅ Saved to data/results/svm_predictions.csv\n",
      "✅ Saved to data/results/indobert_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Setup NLTK and directories\n",
    "def setup_environment():\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    os.makedirs('data/results', exist_ok=True)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text: str) -> str:\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'\\b(?:putusan|nomor|tahun|pengadilan|hakim)\\b', '', text)\n",
    "        text = re.sub(r'uu\\s+no', 'undang-undang nomor', text)\n",
    "        text = re.sub(r'pasal\\s+\\d+', 'pasal', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return ' '.join(word_tokenize(text)) if text else 'empty'\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing text: {e}\")\n",
    "        return 'empty'\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    try:\n",
    "        df = pd.read_csv('data/processed/cases.csv')\n",
    "        texts = df['ringkasan_fakta'].fillna('').apply(preprocess_text).tolist()\n",
    "        case_ids = df['case_id'].tolist()\n",
    "\n",
    "        with open('data/eval/queries.json', 'r', encoding='utf-8') as f:\n",
    "            queries = json.load(f)\n",
    "            case_solutions = {item['case_id']: item.get('solution', '') for item in queries}\n",
    "        \n",
    "        return df, texts, case_ids, queries, case_solutions\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Setup TF-IDF\n",
    "def setup_tfidf(texts: List[str]) -> tuple:\n",
    "    stop_words = [\n",
    "        'dan', 'di', 'dari', 'ke', 'pada', 'dengan', 'untuk', 'yang', 'ini', 'itu',\n",
    "        'adalah', 'tersebut', 'sebagai', 'oleh', 'atau', 'tetapi', 'karena', 'jika',\n",
    "        'dalam', 'bagi', 'tentang', 'melalui', 'serta', 'maka', 'lagi', 'sudah',\n",
    "        'belum', 'hanya', 'saja', 'bahwa', 'apa', 'siapa', 'bagaimana', 'kapan',\n",
    "        'dimana', 'kenapa', 'sejak', 'hingga', 'agar', 'supaya', 'meskipun', 'walau',\n",
    "        'kecuali', 'terhadap', 'antara', 'selain', 'setiap', 'sebelum', 'sesudah'\n",
    "    ]\n",
    "    vectorizer = TfidfVectorizer(max_features=4000, ngram_range=(1, 3), stop_words=stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# Extract features for Logistic Regression and SVM\n",
    "def extract_features(query_vec, doc_vec, query_text: str, doc_text: str) -> np.ndarray:\n",
    "    query_vec = query_vec.toarray()[0]\n",
    "    doc_vec = doc_vec.toarray()[0]\n",
    "    cos_sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "    query_words = set(query_text.split())\n",
    "    doc_words = set(doc_text.split())\n",
    "    overlap = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "    coverage = overlap\n",
    "    return np.concatenate([query_vec, doc_vec, [cos_sim, overlap, coverage]])\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for item in queries:\n",
    "        query = preprocess_text(item['query'])\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        true_id = item['case_id']\n",
    "        try:\n",
    "            true_idx = case_ids.index(true_id)\n",
    "        except ValueError:\n",
    "            print(f\"Case ID {true_id} not found in case_ids\")\n",
    "            continue\n",
    "        true_vec = tfidf_matrix[true_idx]\n",
    "        pos_features = extract_features(query_vec, true_vec, query, texts[true_idx])\n",
    "        neg_indices = [i for i in range(len(case_ids)) if i != true_idx]\n",
    "        neg_samples = np.random.choice(neg_indices, size=min(10, len(neg_indices)), replace=False)\n",
    "        for neg_idx in neg_samples:\n",
    "            neg_vec = tfidf_matrix[neg_idx]\n",
    "            neg_features = extract_features(query_vec, neg_vec, query, texts[neg_idx])\n",
    "            X_train.append(pos_features - neg_features)\n",
    "            y_train.append(1)\n",
    "            X_train.append(neg_features - pos_features)\n",
    "            y_train.append(0)\n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Train models\n",
    "def train_models(X_train, y_train):\n",
    "    param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    \n",
    "    # Logistic Regression\n",
    "    logreg = GridSearchCV(LogisticRegression(max_iter=5000, class_weight='balanced'), param_grid, cv=3, scoring='accuracy')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print(f'Best Logistic Regression Parameters: {logreg.best_params_}')\n",
    "    print(f'Best CV Accuracy (LogReg): {logreg.best_score_:.2f}')\n",
    "    \n",
    "    # SVM\n",
    "    svm = GridSearchCV(LinearSVC(max_iter=5000, class_weight='balanced'), param_grid, cv=3, scoring='accuracy')\n",
    "    svm.fit(X_train, y_train)\n",
    "    print(f'Best SVM Parameters: {svm.best_params_}')\n",
    "    print(f'Best CV Accuracy (SVM): {svm.best_score_:.2f}')\n",
    "    \n",
    "    return logreg, svm\n",
    "\n",
    "# Setup Indo-BERT and BM25\n",
    "def setup_indobert_and_bm25(texts: List[str]):\n",
    "    try:\n",
    "        bi_encoder = SentenceTransformer('indobenchmark/indobert-base-p1')\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        doc_embeddings = bi_encoder.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "        bm25 = BM25Okapi([t.split() for t in texts])\n",
    "        return bi_encoder, cross_encoder, doc_embeddings, bm25\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Indo-BERT/BM25: {e}\")\n",
    "        raise\n",
    "\n",
    "# Retrieval functions\n",
    "def logreg_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, logreg, k: int = 5) -> List[str]:\n",
    "    query = preprocess_text(query)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        doc_vec = tfidf_matrix[i]\n",
    "        features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "        score = logreg.decision_function([features])[0]\n",
    "        scores.append((case_ids[i], score))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in scores[:k]]\n",
    "\n",
    "def svm_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, svm, k: int = 5) -> List[str]:\n",
    "    query = preprocess_text(query)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        doc_vec = tfidf_matrix[i]\n",
    "        features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "        score = svm.decision_function([features])[0]\n",
    "        scores.append((case_ids[i], score))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in scores[:k]]\n",
    "\n",
    "def indobert_retrieve(query: str, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts, k: int = 10, alpha: float = 0.6) -> List[str]:\n",
    "    query = preprocess_text(query)\n",
    "    query_vec = bi_encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sim_scores = cosine_similarity([query_vec], doc_embeddings)[0]\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "    bm25_scores /= np.max(bm25_scores) + 1e-10\n",
    "    combined = alpha * sim_scores + (1 - alpha) * bm25_scores\n",
    "    top_k_idx = np.argsort(combined)[-k:][::-1]\n",
    "    rerank_pairs = [[query, texts[i]] for i in top_k_idx]\n",
    "    rerank_scores = cross_encoder.predict(rerank_pairs)\n",
    "    reranked_idx = np.argsort(rerank_scores)[::-1][:5]\n",
    "    return [case_ids[top_k_idx[i]] for i in reranked_idx]\n",
    "\n",
    "# Predict outcome\n",
    "def predict_outcome(query: str, retrieve_fn, case_solutions) -> tuple:\n",
    "    top_5_ids = retrieve_fn(query)\n",
    "    solutions = [case_solutions.get(cid, '') for cid in top_5_ids]\n",
    "    filtered = [s for s in solutions if s not in ['', None, 'nan']]\n",
    "    predicted = max(set(filtered), key=filtered.count) if filtered else 'Tidak ditemukan'\n",
    "    return predicted, top_5_ids\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    setup_environment()\n",
    "    \n",
    "    # Load data\n",
    "    df, texts, case_ids, queries, case_solutions = load_data()\n",
    "    \n",
    "    # Setup TF-IDF\n",
    "    vectorizer, tfidf_matrix = setup_tfidf(texts)\n",
    "    \n",
    "    # Prepare and train models\n",
    "    X_train, y_train = prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix)\n",
    "    logreg, svm = train_models(X_train, y_train)\n",
    "    \n",
    "    # Setup Indo-BERT and BM25\n",
    "    bi_encoder, cross_encoder, doc_embeddings, bm25 = setup_indobert_and_bm25(texts)\n",
    "    \n",
    "    # Run predictions\n",
    "    results_logreg = []\n",
    "    results_svm = []\n",
    "    results_indobert = []\n",
    "    \n",
    "    for i in tqdm(range(len(df)), desc='Predicting'):\n",
    "        query_text = df.loc[i, 'ringkasan_fakta']\n",
    "        case_id = df.loc[i, 'case_id']\n",
    "        \n",
    "        # Logistic Regression\n",
    "        pred_logreg, top_ids_logreg = predict_outcome(query_text, \n",
    "                                                     lambda q: logreg_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, logreg), \n",
    "                                                     case_solutions)\n",
    "        results_logreg.append({\n",
    "            'query_id': case_id,\n",
    "            'predicted_solution': pred_logreg,\n",
    "            'top_5_case_ids': ', '.join(top_ids_logreg)\n",
    "        })\n",
    "        \n",
    "        # SVM\n",
    "        pred_svm, top_ids_svm = predict_outcome(query_text, \n",
    "                                               lambda q: svm_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, svm), \n",
    "                                               case_solutions)\n",
    "        results_svm.append({\n",
    "            'query_id': case_id,\n",
    "            'predicted_solution': pred_svm,\n",
    "            'top_5_case_ids': ', '.join(top_ids_svm)\n",
    "        })\n",
    "        \n",
    "        # Indo-BERT\n",
    "        pred_indobert, top_ids_indobert = predict_outcome(query_text, \n",
    "                                                         lambda q: indobert_retrieve(q, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts), \n",
    "                                                         case_solutions)\n",
    "        results_indobert.append({\n",
    "            'query_id': case_id,\n",
    "            'predicted_solution': pred_indobert,\n",
    "            'top_5_case_ids': ', '.join(top_ids_indobert)\n",
    "        })\n",
    "    \n",
    "    # Save results\n",
    "    pd.DataFrame(results_logreg).to_csv('data/results/logreg_predictions.csv', index=False, encoding='utf-8')\n",
    "    pd.DataFrame(results_svm).to_csv('data/results/svm_predictions.csv', index=False, encoding='utf-8')\n",
    "    pd.DataFrame(results_indobert).to_csv('data/results/indobert_predictions.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print('✅ Saved to data/results/logreg_predictions.csv')\n",
    "    print('✅ Saved to data/results/svm_predictions.csv')\n",
    "    print('✅ Saved to data/results/indobert_predictions.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
